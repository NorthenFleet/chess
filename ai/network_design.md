# ä¸­å›½è±¡æ£‹AIç¥ç»ç½‘ç»œå®Œæ•´è®¾è®¡æ–¹æ¡ˆ (MCTS + PPO + Actor-Critic)

## ğŸ¯ æ€»ä½“æ¶æ„è®¾è®¡

åŸºäºAlphaZeroçš„æ€æƒ³ï¼Œç»“åˆä¸­å›½è±¡æ£‹çš„ç‰¹ç‚¹ï¼Œè®¾è®¡ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ·±åº¦ç¥ç»ç½‘ç»œã€‚æœ¬è®¾è®¡æ–¹æ¡ˆç»è¿‡æ·±å…¥åˆ†æç°æœ‰ä»£ç å®ç°ï¼Œæä¾›äº†å®Œæ•´çš„ç½‘ç»œæ¶æ„ã€å¥–åŠ±å·¥ç¨‹å’Œè®­ç»ƒç­–ç•¥ã€‚

### æ ¸å¿ƒè®¾è®¡ç†å¿µ
- **Actor-Criticç½‘ç»œæ¶æ„**ï¼šåˆ†ç¦»çš„ç­–ç•¥ç½‘ç»œï¼ˆActorï¼‰å’Œä»·å€¼ç½‘ç»œï¼ˆCriticï¼‰
- **MCTS + PPOè®­ç»ƒ**ï¼šè’™ç‰¹å¡æ´›æ ‘æœç´¢æŒ‡å¯¼PPOç­–ç•¥ä¼˜åŒ–
- **æ®‹å·®å·ç§¯ç½‘ç»œ**ï¼šç”¨äºæ£‹ç›˜ç‰¹å¾æå–ï¼Œè§£å†³æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
- **æ³¨æ„åŠ›æœºåˆ¶**ï¼šå¢å¼ºå¯¹å…³é”®ä½ç½®å’Œå¨èƒçš„æ„ŸçŸ¥èƒ½åŠ›
- **è‡ªå¯¹å¼ˆè®­ç»ƒ**ï¼šé€šè¿‡MCTS + ç¥ç»ç½‘ç»œè¿›è¡Œå¼ºåŒ–å­¦ä¹ 
- **åˆ†é˜¶æ®µè®­ç»ƒ**ï¼šä»ç›‘ç£å­¦ä¹ åˆ°å¼ºåŒ–å­¦ä¹ çš„æ¸è¿›å¼è®­ç»ƒ

## ğŸ“Š ç½‘ç»œè¾“å…¥è®¾è®¡

### è¾“å…¥ç»´åº¦ï¼š`(batch_size, 14, 10, 9)`

#### é€šé“è®¾è®¡ (14ä¸ªé€šé“)
åŸºäºç°æœ‰ç¼–ç å™¨å®ç°ï¼Œé‡‡ç”¨æ£‹å­ç±»å‹åˆ†ç¦»ç¼–ç ï¼š

**çº¢æ–¹æ£‹å­é€šé“ (0-6)**ï¼š
- é€šé“0ï¼šå¸… (GENERAL)
- é€šé“1ï¼šå£« (ADVISOR)  
- é€šé“2ï¼šè±¡ (ELEPHANT)
- é€šé“3ï¼šè½¦ (CHARIOT)
- é€šé“4ï¼šé©¬ (HORSE)
- é€šé“5ï¼šç‚® (CANNON)
- é€šé“6ï¼šå…µ (SOLDIER)

**é»‘æ–¹æ£‹å­é€šé“ (7-13)**ï¼š
- é€šé“7ï¼šå°† (GENERAL)
- é€šé“8ï¼šå£« (ADVISOR)
- é€šé“9ï¼šè±¡ (ELEPHANT)
- é€šé“10ï¼šè½¦ (CHARIOT)
- é€šé“11ï¼šé©¬ (HORSE)
- é€šé“12ï¼šç‚® (CANNON)
- é€šé“13ï¼šå…µ (SOLDIER)

#### æ£‹ç›˜å°ºå¯¸ï¼š`10 Ã— 9`
- 10è¡Œï¼šå¯¹åº”ä¸­å›½è±¡æ£‹çš„10æ¡æ¨ªçº¿
- 9åˆ—ï¼šå¯¹åº”ä¸­å›½è±¡æ£‹çš„9æ¡ç«–çº¿

### è¾“å…¥ç¼–ç æ–¹å¼
```python
# æ¯ä¸ªé€šé“çš„ç¼–ç 
# 1.0: è¯¥ä½ç½®æœ‰å¯¹åº”æ£‹å­
# 0.0: è¯¥ä½ç½®æ— å¯¹åº”æ£‹å­

# ç¤ºä¾‹ï¼šçº¢è½¦åœ¨(0,0)ä½ç½®
input_tensor[0, 3, 0, 0] = 1.0  # ç¬¬3ä¸ªé€šé“(çº¢è½¦)ï¼Œä½ç½®(0,0)

# è§†è§’è½¬æ¢ï¼šé»‘æ–¹è§†è§’æ—¶è‡ªåŠ¨ç¿»è½¬æ£‹ç›˜
if current_player == "black":
    encoded = flip_board_perspective(encoded)
```

**è®¾è®¡ä¼˜åŠ¿**ï¼š
- æ£‹å­ç±»å‹åˆ†ç¦»ï¼Œä¾¿äºç½‘ç»œå­¦ä¹ ä¸åŒæ£‹å­çš„ç§»åŠ¨æ¨¡å¼
- çº¢é»‘åˆ†ç¦»ï¼Œæ˜ç¡®åŒºåˆ†æ•Œæˆ‘æ£‹å­
- è§†è§’ç»Ÿä¸€ï¼Œå§‹ç»ˆä»å½“å‰ç©å®¶è§’åº¦ç¼–ç 

## ğŸ—ï¸ ç½‘ç»œæ¶æ„è¯¦ç»†è®¾è®¡

### Actor-Criticç½‘ç»œæ¶æ„

#### Actorç½‘ç»œï¼ˆç­–ç•¥ç½‘ç»œï¼‰
```python
class ActorNetwork(nn.Module):
    def __init__(self, 
                 input_channels=14,
                 hidden_channels=256,
                 num_residual_blocks=20,
                 num_attention_heads=8,
                 action_space_size=2086):
        # åˆå§‹å·ç§¯å±‚
        self.input_conv = nn.Conv2d(input_channels, hidden_channels, 3, padding=1)
        self.input_bn = nn.BatchNorm2d(hidden_channels)
        
        # æ®‹å·®å—
        self.residual_blocks = nn.ModuleList([
            ResidualBlock(hidden_channels) for _ in range(num_residual_blocks)
        ])
        
        # è‡ªæ³¨æ„åŠ›æœºåˆ¶
        self.attention = SelfAttention(hidden_channels, num_attention_heads)
        
        # ç­–ç•¥å¤´
        self.policy_conv = nn.Conv2d(hidden_channels, 32, 1)
        self.policy_fc = nn.Linear(32 * 10 * 9, action_space_size)
    
    def forward(self, x, action_mask=None):
        # ç‰¹å¾æå–
        out = F.relu(self.input_bn(self.input_conv(x)))
        for block in self.residual_blocks:
            out = block(out)
        out = self.attention(out)
        
        # ç­–ç•¥è¾“å‡º
        policy_out = F.relu(self.policy_conv(out))
        policy_logits = self.policy_fc(policy_out.view(out.size(0), -1))
        
        # åº”ç”¨åŠ¨ä½œæ©ç 
        if action_mask is not None:
            policy_logits += (action_mask - 1) * 1e9
        
        return F.softmax(policy_logits, dim=1), policy_logits
```

#### Criticç½‘ç»œï¼ˆä»·å€¼ç½‘ç»œï¼‰
```python
class CriticNetwork(nn.Module):
    def __init__(self, 
                 input_channels=14,
                 hidden_channels=256,
                 num_residual_blocks=20,
                 num_attention_heads=8):
        # ä¸Actorå…±äº«ç›¸åŒçš„ç‰¹å¾æå–ç»“æ„
        self.input_conv = nn.Conv2d(input_channels, hidden_channels, 3, padding=1)
        self.input_bn = nn.BatchNorm2d(hidden_channels)
        
        self.residual_blocks = nn.ModuleList([
            ResidualBlock(hidden_channels) for _ in range(num_residual_blocks)
        ])
        
        self.attention = SelfAttention(hidden_channels, num_attention_heads)
        
        # ä»·å€¼å¤´
        self.value_conv = nn.Conv2d(hidden_channels, 1, 1)
        self.value_fc1 = nn.Linear(10 * 9, 256)
        self.value_fc2 = nn.Linear(256, 1)
    
    def forward(self, x):
        # ç‰¹å¾æå–
        out = F.relu(self.input_bn(self.input_conv(x)))
        for block in self.residual_blocks:
            out = block(out)
        out = self.attention(out)
        
        # ä»·å€¼è¾“å‡º
        value_out = F.relu(self.value_conv(out))
        value_out = value_out.view(out.size(0), -1)
        value = torch.tanh(self.value_fc2(F.relu(self.value_fc1(value_out))))
        
        return value
```

**è®¾è®¡ä¼˜åŠ¿**ï¼š
- **ç‹¬ç«‹ä¼˜åŒ–**ï¼šActorå’ŒCriticå¯ä»¥ç‹¬ç«‹ä¼˜åŒ–ï¼Œé¿å…ç›®æ ‡å†²çª
- **ä¸“é—¨åŒ–è®¾è®¡**ï¼šæ¯ä¸ªç½‘ç»œä¸“æ³¨äºè‡ªå·±çš„ä»»åŠ¡ï¼ˆç­–ç•¥vsä»·å€¼ï¼‰
- **çµæ´»æ€§**ï¼šå¯ä»¥é€‰æ‹©å…±äº«ä¸»å¹²ç½‘ç»œæˆ–å®Œå…¨ç‹¬ç«‹
- **ç¨³å®šæ€§**ï¼šå‡å°‘è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç›¸äº’å¹²æ‰°

### 1. ç‰¹å¾æå–å±‚ (Convolutional Backbone)

#### åˆå§‹å·ç§¯å±‚
```python
Conv2D(in_channels=14, out_channels=256, kernel_size=3, padding=1)
BatchNorm2d(256)
ReLU()
```

**è®¾è®¡ç†ç”±**ï¼š
- 14â†’256é€šé“æ‰©å±•ï¼Œæä¾›è¶³å¤Ÿçš„ç‰¹å¾è¡¨è¾¾èƒ½åŠ›
- 3Ã—3å·ç§¯æ ¸æ•è·å±€éƒ¨æ£‹å­å…³ç³»
- BatchNormåŠ é€Ÿæ”¶æ•›ï¼ŒReLUæä¾›éçº¿æ€§

#### æ®‹å·®å— Ã— 20å±‚
```python
class ResidualBlock:
    def __init__(self, channels=256):
        self.conv1 = Conv2D(256, 256, kernel_size=3, padding=1, bias=False)
        self.bn1 = BatchNorm2d(256)
        self.conv2 = Conv2D(256, 256, kernel_size=3, padding=1, bias=False)
        self.bn2 = BatchNorm2d(256)
    
    def forward(self, x):
        residual = x
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += residual  # æ®‹å·®è¿æ¥
        return F.relu(out)
```

**è®¾è®¡ç†ç”±**ï¼š
- 20ä¸ªæ®‹å·®å—æä¾›è¶³å¤Ÿçš„ç½‘ç»œæ·±åº¦ï¼ˆçº¦40å±‚å·ç§¯ï¼‰
- 256ä¸ªç‰¹å¾é€šé“å¹³è¡¡è¡¨è¾¾èƒ½åŠ›å’Œè®¡ç®—æ•ˆç‡
- æ®‹å·®è¿æ¥è§£å†³æ·±åº¦ç½‘ç»œçš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
- æ— åç½®å·ç§¯é…åˆBatchNormï¼Œå‡å°‘å‚æ•°é‡

### 2. æ³¨æ„åŠ›æœºåˆ¶å±‚

#### è‡ªæ³¨æ„åŠ›æ¨¡å—
```python
class SelfAttention:
    def __init__(self, channels=256, num_heads=8):
        self.num_heads = num_heads
        self.head_dim = channels // num_heads  # 32
        
        self.query = Linear(channels, channels)
        self.key = Linear(channels, channels)
        self.value = Linear(channels, channels)
        self.output_proj = Linear(channels, channels)
    
    def forward(self, x):
        # x: (batch, 256, 10, 9)
        batch_size, channels, height, width = x.shape
        
        # é‡å¡‘ä¸ºåºåˆ—æ ¼å¼
        x_flat = x.view(batch_size, channels, -1).transpose(1, 2)  # (batch, 90, 256)
        
        # å¤šå¤´æ³¨æ„åŠ›è®¡ç®—
        Q = self.query(x_flat).view(batch_size, -1, self.num_heads, self.head_dim)
        K = self.key(x_flat).view(batch_size, -1, self.num_heads, self.head_dim)
        V = self.value(x_flat).view(batch_size, -1, self.num_heads, self.head_dim)
        
        # æ³¨æ„åŠ›æƒé‡è®¡ç®—
        attention_weights = torch.softmax(
            torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim), 
            dim=-1
        )
        
        # åŠ æƒæ±‚å’Œ
        attended = torch.matmul(attention_weights, V)
        
        # è¾“å‡ºæŠ•å½±å¹¶é‡å¡‘å›åŸå§‹å½¢çŠ¶
        output = self.output_proj(attended.view(batch_size, -1, channels))
        return output.transpose(1, 2).view(batch_size, channels, height, width)
```

**è®¾è®¡ç†ç”±**ï¼š
- 8ä¸ªæ³¨æ„åŠ›å¤´ï¼Œæ¯å¤´32ç»´ï¼Œå¹³è¡¡è®¡ç®—å¤æ‚åº¦å’Œè¡¨è¾¾èƒ½åŠ›
- å…¨å±€æ³¨æ„åŠ›æœºåˆ¶ï¼Œè®©ç½‘ç»œå…³æ³¨æ£‹ç›˜ä¸Šçš„å…³é”®ä½ç½®å…³ç³»
- å¢å¼ºå¯¹å¨èƒã€ä¿æŠ¤ã€æˆ˜æœ¯ç»„åˆçš„æ„ŸçŸ¥èƒ½åŠ›

### 3. ç­–ç•¥ç½‘ç»œå¤´ (Policy Head)

#### ç½‘ç»œç»“æ„
```python
class PolicyHead:
    def __init__(self, input_channels=256, action_space_size=2086):
        self.conv = Conv2D(input_channels, 32, kernel_size=1, bias=False)
        self.bn = BatchNorm2d(32)
        self.fc = Linear(32 * 10 * 9, action_space_size)
    
    def forward(self, x):
        # ç‰¹å¾å‹ç¼©
        out = F.relu(self.bn(self.conv(x)))  # (batch, 32, 10, 9)
        
        # å±•å¹³
        out = out.view(out.size(0), -1)  # (batch, 2880)
        
        # å…¨è¿æ¥è¾“å‡º
        out = self.fc(out)  # (batch, 2086)
        
        # Softmaxå¾—åˆ°æ¦‚ç‡åˆ†å¸ƒ
        return F.softmax(out, dim=1)
```

#### åŠ¨ä½œç©ºé—´è®¾è®¡ï¼š2086ç»´
```python
def encode_action(from_pos, to_pos):
    """
    åŠ¨ä½œç¼–ç ï¼š90ä¸ªèµ·å§‹ä½ç½® Ã— æœ€å¤š89ä¸ªç›®æ ‡ä½ç½® = æœ€å¤š8010ç§ç»„åˆ
    å®é™…åˆæ³•åŠ¨ä½œçº¦2086ç§ï¼ˆè€ƒè™‘è±¡æ£‹è§„åˆ™é™åˆ¶ï¼‰
    """
    from_idx = from_pos[0] * 9 + from_pos[1]  # 0-89
    to_idx = to_pos[0] * 9 + to_pos[1]        # 0-89
    
    # é¿å…è‡ªç§»åŠ¨
    if to_idx >= from_idx:
        to_idx += 1
        
    action_idx = from_idx * 89 + to_idx
    return action_idx
```

**è®¾è®¡ç†ç”±**ï¼š
- 1Ã—1å·ç§¯é™ç»´ï¼Œå‡å°‘å‚æ•°é‡åŒæ—¶ä¿æŒç©ºé—´ä¿¡æ¯
- 2086ç»´è¾“å‡ºè¦†ç›–æ‰€æœ‰å¯èƒ½çš„åˆæ³•ç§»åŠ¨
- Softmaxè¾“å‡ºæ¦‚ç‡åˆ†å¸ƒï¼Œä¾¿äºMCTSé‡‡æ ·

### 4. ä»·å€¼ç½‘ç»œå¤´ (Value Head)

#### ç½‘ç»œç»“æ„
```python
class ValueHead:
    def __init__(self, input_channels=256):
        self.conv = Conv2D(input_channels, 1, kernel_size=1, bias=False)
        self.bn = BatchNorm2d(1)
        self.fc1 = Linear(10 * 9, 256)
        self.fc2 = Linear(256, 1)
    
    def forward(self, x):
        # ç‰¹å¾å‹ç¼©åˆ°å•é€šé“
        out = F.relu(self.bn(self.conv(x)))  # (batch, 1, 10, 9)
        
        # å±•å¹³
        out = out.view(out.size(0), -1)  # (batch, 90)
        
        # ä¸¤å±‚å…¨è¿æ¥
        out = F.relu(self.fc1(out))  # (batch, 256)
        out = self.fc2(out)          # (batch, 1)
        
        # Tanhæ¿€æ´»ï¼Œè¾“å‡ºèŒƒå›´ [-1, 1]
        return torch.tanh(out)
```

#### è¾“å‡ºå«ä¹‰
- **è¾“å‡ºèŒƒå›´**ï¼š[-1, 1]
- **+1**ï¼šå½“å‰ç©å®¶å¿…èƒœå±€é¢
- **0**ï¼šå‡åŠ¿å±€é¢  
- **-1**ï¼šå½“å‰ç©å®¶å¿…è´¥å±€é¢

**è®¾è®¡ç†ç”±**ï¼š
- å•é€šé“å‹ç¼©æå–å…¨å±€ç‰¹å¾
- ä¸¤å±‚å…¨è¿æ¥æä¾›è¶³å¤Ÿçš„éçº¿æ€§å˜æ¢
- Tanhæ¿€æ´»ç¡®ä¿è¾“å‡ºåœ¨åˆç†èŒƒå›´å†…

## ğŸ¯ PPOè®­ç»ƒç®—æ³•

### PPO (Proximal Policy Optimization) æ ¸å¿ƒåŸç†

PPOæ˜¯ä¸€ç§ç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼Œé€šè¿‡é™åˆ¶ç­–ç•¥æ›´æ–°çš„å¹…åº¦æ¥ä¿è¯è®­ç»ƒç¨³å®šæ€§ï¼š

```python
class PPOTrainer:
    def __init__(self, actor_critic_network, config):
        self.network = actor_critic_network
        self.config = config
        self.optimizer = optim.Adam(network.parameters(), lr=config.learning_rate)
    
    def compute_policy_loss(self, batch):
        # è·å–å½“å‰ç­–ç•¥çš„logæ¦‚ç‡
        log_probs, _, entropy = self.network.evaluate_actions(
            batch['observations'], batch['actions'], batch['action_masks']
        )
        
        # è®¡ç®—é‡è¦æ€§é‡‡æ ·æ¯”ç‡
        ratio = torch.exp(log_probs - batch['old_log_probs'])
        
        # PPOè£å‰ªæŸå¤±
        surr1 = ratio * batch['advantages']
        surr2 = torch.clamp(ratio, 1-Îµ, 1+Îµ) * batch['advantages']
        policy_loss = -torch.min(surr1, surr2).mean()
        
        return policy_loss, entropy.mean()
    
    def compute_value_loss(self, batch):
        _, _, values = self.network(batch['observations'])
        
        # è£å‰ªä»·å€¼æŸå¤±
        value_pred_clipped = batch['old_values'] + torch.clamp(
            values - batch['old_values'], -Îµ, Îµ
        )
        value_loss1 = (values - batch['returns']).pow(2)
        value_loss2 = (value_pred_clipped - batch['returns']).pow(2)
        
        return torch.max(value_loss1, value_loss2).mean()
```

### ä¼˜åŠ¿å‡½æ•°è®¡ç®— (GAE)

ä½¿ç”¨å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡(GAE)æ¥å‡å°‘æ–¹å·®ï¼š

```python
def compute_gae_advantages(rewards, values, dones, gamma=0.99, gae_lambda=0.95):
    advantages = np.zeros_like(rewards)
    last_gae_lam = 0
    
    for step in reversed(range(len(rewards))):
        if step == len(rewards) - 1:
            next_non_terminal = 1.0 - dones[step]
            next_value = 0
        else:
            next_non_terminal = 1.0 - dones[step]
            next_value = values[step + 1]
        
        delta = rewards[step] + gamma * next_value * next_non_terminal - values[step]
        advantages[step] = last_gae_lam = delta + gamma * gae_lambda * next_non_terminal * last_gae_lam
    
    returns = advantages + values
    return advantages, returns
```

### PPOè®­ç»ƒæµç¨‹

1. **æ•°æ®æ”¶é›†**ï¼šé€šè¿‡MCTSè‡ªå¯¹å¼ˆæ”¶é›†ç»éªŒ
2. **ä¼˜åŠ¿è®¡ç®—**ï¼šä½¿ç”¨GAEè®¡ç®—ä¼˜åŠ¿å‡½æ•°
3. **ç­–ç•¥æ›´æ–°**ï¼šä½¿ç”¨PPOæŸå¤±æ›´æ–°Actorç½‘ç»œ
4. **ä»·å€¼æ›´æ–°**ï¼šä½¿ç”¨MSEæŸå¤±æ›´æ–°Criticç½‘ç»œ
5. **é‡å¤è¿­ä»£**ï¼šæŒç»­ä¼˜åŒ–ç›´åˆ°æ”¶æ•›

**PPOä¼˜åŠ¿**ï¼š
- **ç¨³å®šæ€§**ï¼šè£å‰ªæœºåˆ¶é˜²æ­¢ç­–ç•¥æ›´æ–°è¿‡å¤§
- **æ ·æœ¬æ•ˆç‡**ï¼šå¯ä»¥é‡å¤ä½¿ç”¨ç»éªŒæ•°æ®
- **ç®€å•æ€§**ï¼šç›¸æ¯”TRPOç­‰æ–¹æ³•æ›´å®¹æ˜“å®ç°
- **é²æ£’æ€§**ï¼šå¯¹è¶…å‚æ•°ä¸æ•æ„Ÿ

## ğŸ å¥–åŠ±å·¥ç¨‹è®¾è®¡

### 1. åŸºç¡€å¥–åŠ±å‡½æ•°

#### æ¸¸æˆç»“æœå¥–åŠ±
```python
def get_game_result_reward(game_state, player):
    """æ¸¸æˆç»“æŸæ—¶çš„åŸºç¡€å¥–åŠ±"""
    if game_state.game_over:
        if game_state.winner == player:
            return 1.0    # èƒœåˆ©
        elif game_state.winner is None:
            return 0.0    # å¹³å±€
        else:
            return -1.0   # å¤±è´¥
    return 0.0  # æ¸¸æˆæœªç»“æŸ
```

#### æè´¨ä»·å€¼å¥–åŠ±
```python
PIECE_VALUES = {
    PieceType.GENERAL: 1000,   # å°†/å¸…
    PieceType.CHARIOT: 9,      # è½¦
    PieceType.CANNON: 4.5,     # ç‚®
    PieceType.HORSE: 4,        # é©¬
    PieceType.ADVISOR: 2,      # å£«
    PieceType.ELEPHANT: 2,     # è±¡
    PieceType.SOLDIER: 1,      # å…µ/å’
}

def calculate_material_advantage(board, player):
    """è®¡ç®—æè´¨ä¼˜åŠ¿"""
    my_value = sum(PIECE_VALUES[piece.type] 
                   for piece in board.get_pieces(player))
    opponent_value = sum(PIECE_VALUES[piece.type] 
                        for piece in board.get_pieces(get_opponent(player)))
    
    return (my_value - opponent_value) / 100.0  # å½’ä¸€åŒ–
```

### 2. ä½ç½®ä»·å€¼å¥–åŠ±

#### æ£‹å­ä½ç½®è¡¨
```python
# å…µçš„ä½ç½®ä»·å€¼è¡¨ï¼ˆçº¢æ–¹è§†è§’ï¼‰
SOLDIER_POSITION_VALUES = np.array([
    [0, 0, 0, 0, 0, 0, 0, 0, 0],  # ç¬¬0è¡Œ
    [0, 0, 0, 0, 0, 0, 0, 0, 0],  # ç¬¬1è¡Œ
    [0, 0, 0, 0, 0, 0, 0, 0, 0],  # ç¬¬2è¡Œ
    [0, 0, 1, 0, 3, 0, 1, 0, 0],  # ç¬¬3è¡Œï¼ˆå…µçº¿ï¼‰
    [0, 0, 2, 0, 4, 0, 2, 0, 0],  # ç¬¬4è¡Œ
    [3, 0, 4, 0, 5, 0, 4, 0, 3],  # ç¬¬5è¡Œï¼ˆæ²³ç•Œï¼‰
    [3, 0, 5, 1, 6, 1, 5, 0, 3],  # ç¬¬6è¡Œ
    [4, 2, 6, 3, 7, 3, 6, 2, 4],  # ç¬¬7è¡Œ
    [4, 4, 7, 5, 8, 5, 7, 4, 4],  # ç¬¬8è¡Œ
    [5, 5, 8, 6, 9, 6, 8, 5, 5],  # ç¬¬9è¡Œ
]) / 10.0  # å½’ä¸€åŒ–

def calculate_position_value(board, player):
    """è®¡ç®—ä½ç½®ä»·å€¼"""
    total_value = 0.0
    for piece in board.get_pieces(player):
        pos = piece.position
        if piece.type == PieceType.SOLDIER:
            if player == "red":
                total_value += SOLDIER_POSITION_VALUES[pos.row, pos.col]
            else:
                # é»‘æ–¹è§†è§’ç¿»è½¬
                total_value += SOLDIER_POSITION_VALUES[9-pos.row, pos.col]
        # å…¶ä»–æ£‹å­çš„ä½ç½®ä»·å€¼è¡¨...
    
    return total_value
```

### 3. æˆ˜æœ¯å¥–åŠ±

#### å¨èƒå’Œä¿æŠ¤
```python
def calculate_tactical_reward(board, rule, player):
    """è®¡ç®—æˆ˜æœ¯å¥–åŠ±"""
    reward = 0.0
    
    # å¨èƒå¯¹æ–¹æ£‹å­
    for piece in board.get_pieces(player):
        valid_moves = rule.get_valid_moves_for_piece(piece)
        for move in valid_moves:
            target_piece = board.get_piece_at(move.to_pos)
            if target_piece and target_piece.side != player:
                # å¨èƒä»·å€¼ = ç›®æ ‡æ£‹å­ä»·å€¼ Ã— å¨èƒç³»æ•°
                threat_value = PIECE_VALUES[target_piece.type] * 0.1
                reward += threat_value
    
    # ä¿æŠ¤å·±æ–¹æ£‹å­
    for piece in board.get_pieces(player):
        if is_piece_protected(board, rule, piece):
            protection_value = PIECE_VALUES[piece.type] * 0.05
            reward += protection_value
    
    return reward / 100.0  # å½’ä¸€åŒ–
```

#### æ§åˆ¶ä¸­å¿ƒ
```python
def calculate_center_control(board, player):
    """è®¡ç®—ä¸­å¿ƒæ§åˆ¶å¥–åŠ±"""
    center_positions = [
        Position(4, 3), Position(4, 4), Position(4, 5),
        Position(5, 3), Position(5, 4), Position(5, 5),
    ]
    
    control_score = 0.0
    for pos in center_positions:
        piece = board.get_piece_at(pos)
        if piece and piece.side == player:
            control_score += 1.0
        
        # è®¡ç®—å¯¹è¯¥ä½ç½®çš„æ§åˆ¶åŠ›
        attackers = count_attackers(board, pos, player)
        control_score += attackers * 0.2
    
    return control_score / 10.0  # å½’ä¸€åŒ–
```

### 4. ç»¼åˆå¥–åŠ±å‡½æ•°

```python
def calculate_comprehensive_reward(game_state, player, move=None):
    """ç»¼åˆå¥–åŠ±å‡½æ•°"""
    board = game_state.board
    rule = game_state.rule
    
    # åŸºç¡€æ¸¸æˆç»“æœå¥–åŠ±
    game_reward = get_game_result_reward(game_state, player)
    if game_reward != 0:  # æ¸¸æˆç»“æŸ
        return game_reward
    
    # å„é¡¹å­å¥–åŠ±
    material_reward = calculate_material_advantage(board, player)
    position_reward = calculate_position_value(board, player)
    tactical_reward = calculate_tactical_reward(board, rule, player)
    center_reward = calculate_center_control(board, player)
    
    # ç§»åŠ¨å¥–åŠ±ï¼ˆå¦‚æœæä¾›äº†ç§»åŠ¨ï¼‰
    move_reward = 0.0
    if move:
        move_reward = calculate_move_quality(board, rule, move, player)
    
    # åŠ æƒç»„åˆ
    total_reward = (
        material_reward * 0.4 +      # æè´¨æœ€é‡è¦
        position_reward * 0.2 +      # ä½ç½®ä»·å€¼
        tactical_reward * 0.2 +      # æˆ˜æœ¯ä»·å€¼
        center_reward * 0.1 +        # ä¸­å¿ƒæ§åˆ¶
        move_reward * 0.1            # ç§»åŠ¨è´¨é‡
    )
    
    return np.clip(total_reward, -1.0, 1.0)  # é™åˆ¶åœ¨[-1,1]èŒƒå›´
```

## ğŸ”„ MCTS + PPO è®­ç»ƒæµç¨‹

### æ•´ä½“è®­ç»ƒæ¶æ„

```python
class MCTSPPOTrainer:
    def __init__(self, config):
        self.network = ActorCriticNetwork()
        self.ppo_trainer = PPOTrainer(self.network)
        self.mcts = MCTS(self.network)
        self.buffer = RolloutBuffer()
    
    def train_iteration(self):
        # 1. è‡ªå¯¹å¼ˆæ•°æ®æ”¶é›†
        game_results = self.collect_self_play_data()
        
        # 2. æ·»åŠ åˆ°ç»éªŒç¼“å†²åŒº
        for result in game_results:
            self.add_game_to_buffer(result)
        
        # 3. PPOè®­ç»ƒ
        if self.buffer.size >= min_buffer_size:
            self.buffer.compute_advantages_and_returns()
            ppo_stats = self.ppo_trainer.update(self.buffer)
            self.buffer.clear()
        
        # 4. ç½‘ç»œè¯„ä¼°
        if iteration % eval_interval == 0:
            self.evaluate_network()
```

### MCTSæŒ‡å¯¼çš„æ•°æ®æ”¶é›†

```python
def collect_self_play_data(self, num_games):
    results = []
    mcts = self.create_mcts(self.network)
    
    for game_idx in range(num_games):
        board = ChessBoard()
        game_result = GameResult()
        
        while not board.is_game_over():
            # MCTSæœç´¢è·å¾—åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ
            if game_idx < num_games * 0.3:  # å‰30%ä½¿ç”¨é«˜æ¸©åº¦
                temperature = 1.0
                mcts_probs = mcts.search_with_noise(board)
            else:
                temperature = 0.1
                mcts_probs = mcts.search(board)
            
            # æ ¹æ®MCTSæ¦‚ç‡é€‰æ‹©åŠ¨ä½œ
            action = self.sample_action(mcts_probs, temperature)
            
            # è®°å½•çŠ¶æ€ã€åŠ¨ä½œã€MCTSæ¦‚ç‡
            state = self.encoder.encode_board(board)
            game_result.states.append(state)
            game_result.actions.append(action)
            game_result.mcts_probs.append(mcts_probs)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            board.make_move(action)
        
        # è®¡ç®—å¥–åŠ±
        game_result.rewards = self.compute_rewards(game_result)
        results.append(game_result)
    
    return results
```

### è®­ç»ƒæ•°æ®è½¬æ¢

```python
def add_game_to_buffer(self, game_result):
    for i, (state, action, reward) in enumerate(zip(
        game_result.states, game_result.actions, game_result.rewards
    )):
        # è·å–ç½‘ç»œé¢„æµ‹çš„ä»·å€¼
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            _, _, value = self.network(state_tensor)
            
            # è®¡ç®—åŠ¨ä½œæ¦‚ç‡å’Œlogæ¦‚ç‡
            action_probs, _, _ = self.network(state_tensor, action_mask)
            log_prob = torch.log(action_probs[0, action] + 1e-8)
        
        # æ·»åŠ åˆ°ç¼“å†²åŒº
        done = (i == len(game_result.states) - 1)
        self.buffer.add(state, action, reward, value.item(), 
                       log_prob.item(), done, action_mask)
```

### å…³é”®è®¾è®¡ç‰¹ç‚¹

1. **MCTSæŒ‡å¯¼å­¦ä¹ **ï¼š
   - MCTSæä¾›é«˜è´¨é‡çš„åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒä½œä¸ºç›‘ç£ä¿¡å·
   - ç½‘ç»œå­¦ä¹ æ¨¡ä»¿MCTSçš„å†³ç­–æ¨¡å¼
   - é€æ­¥å‡å°‘å¯¹MCTSçš„ä¾èµ–

2. **æ¸©åº¦è°ƒèŠ‚ç­–ç•¥**ï¼š
   - è®­ç»ƒåˆæœŸä½¿ç”¨é«˜æ¸©åº¦å¢åŠ æ¢ç´¢
   - è®­ç»ƒåæœŸé™ä½æ¸©åº¦æé«˜ç¡®å®šæ€§
   - å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨

3. **å™ªå£°æ³¨å…¥**ï¼š
   - åœ¨MCTSæœç´¢ä¸­æ·»åŠ ç‹„åˆ©å…‹é›·å™ªå£°
   - å¢åŠ ç­–ç•¥å¤šæ ·æ€§
   - é˜²æ­¢è¿‡æ—©æ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜

4. **æ¸è¿›å¼è®­ç»ƒ**ï¼š
   - ä»æ¨¡ä»¿MCTSå¼€å§‹
   - é€æ­¥æå‡ç½‘ç»œè‡ªä¸»å†³ç­–èƒ½åŠ›
   - æœ€ç»ˆè¶…è¶Šçº¯MCTSæ€§èƒ½

## ğŸš€ é˜¶æ®µè®­ç»ƒæ–¹æ¡ˆ

### ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€ç½‘ç»œè®­ç»ƒ (1-100è½®)
- **ç›®æ ‡**ï¼šå»ºç«‹åŸºç¡€çš„æ£‹å±€ç†è§£èƒ½åŠ›
- **æ–¹æ³•**ï¼šä½¿ç”¨å†å²æ£‹è°±è¿›è¡Œç›‘ç£å­¦ä¹ 
- **ç½‘ç»œ**ï¼šActor-Criticç½‘ç»œï¼Œå…±äº«ä¸»å¹²
- **æ•°æ®**ï¼šä¸“ä¸šæ£‹æ‰‹å¯¹å±€æ•°æ®
- **è¯„ä¼°**ï¼šä¸éšæœºç­–ç•¥å¯¹æ¯”

#### æ•°æ®æ¥æº
```python
# ä¸“ä¸šæ£‹è°±æ•°æ®
training_data = {
    "professional_games": 10000,    # ä¸“ä¸šæ¯”èµ›æ£‹è°±
    "master_games": 5000,          # å¤§å¸ˆçº§å¯¹å±€
    "online_games": 20000,         # é«˜è´¨é‡ç½‘ç»œå¯¹å±€
}

# æ•°æ®é¢„å¤„ç†
def preprocess_game_data(pgn_files):
    """
    å°†PGNæ ¼å¼æ£‹è°±è½¬æ¢ä¸ºè®­ç»ƒæ ·æœ¬
    æ¯ä¸ªä½ç½®ç”Ÿæˆï¼š(æ£‹ç›˜çŠ¶æ€, ä¸“å®¶ç§»åŠ¨, æœ€ç»ˆç»“æœ)
    """
    examples = []
    for game in pgn_files:
        positions = extract_positions(game)
        result = get_game_result(game)
        
        for pos, expert_move in positions:
            board_state = encoder.encode_board(pos.board, pos.player)
            action_prob = create_expert_action_distribution(expert_move)
            value = calculate_position_value_from_result(result, pos.player)
            
            examples.append(TrainingExample(
                board_state=board_state,
                action_probs=action_prob,
                value=value,
                player=pos.player
            ))
    
    return examples
```

#### è®­ç»ƒé…ç½®
```python
supervised_config = {
    "learning_rate": 0.01,          # è¾ƒé«˜å­¦ä¹ ç‡å¿«é€Ÿå­¦ä¹ 
    "batch_size": 64,               # è¾ƒå¤§æ‰¹æ¬¡
    "epochs": 100,                  # å……åˆ†è®­ç»ƒ
    "weight_decay": 1e-4,
    "lr_scheduler": "cosine",       # ä½™å¼¦é€€ç«
    "early_stopping": True,         # é˜²æ­¢è¿‡æ‹Ÿåˆ
    "validation_split": 0.2,
}
```

### ç¬¬äºŒé˜¶æ®µï¼šMCTSæŒ‡å¯¼è®­ç»ƒ (101-500è½®)
- **ç›®æ ‡**ï¼šå­¦ä¹ MCTSçš„æœç´¢ç­–ç•¥
- **æ–¹æ³•**ï¼šMCTS + PPOè”åˆè®­ç»ƒ
- **ç­–ç•¥**ï¼š
  - ä½¿ç”¨MCTSç”Ÿæˆé«˜è´¨é‡è®­ç»ƒæ•°æ®
  - PPOä¼˜åŒ–Actor-Criticç½‘ç»œ
  - é€æ­¥å‡å°‘MCTSæœç´¢æ·±åº¦
- **æ¸©åº¦è°ƒèŠ‚**ï¼šä»1.0é€æ­¥é™è‡³0.1
- **è¯„ä¼°**ï¼šä¸çº¯MCTSå¯¹æ¯”

#### AlphaZeroå¼è®­ç»ƒå¾ªç¯
```python
def alphazero_training_iteration():
    """AlphaZeroè®­ç»ƒè¿­ä»£"""
    
    # 1. è‡ªå¯¹å¼ˆç”Ÿæˆæ•°æ®
    print("ç”Ÿæˆè‡ªå¯¹å¼ˆæ•°æ®...")
    self_play_examples = []
    for game_idx in range(num_self_play_games):
        game_examples = generate_self_play_game()
        self_play_examples.extend(game_examples)
    
    # 2. è®­ç»ƒç¥ç»ç½‘ç»œ
    print("è®­ç»ƒç¥ç»ç½‘ç»œ...")
    train_network(self_play_examples)
    
    # 3. è¯„ä¼°æ–°æ¨¡å‹
    print("è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
    win_rate = evaluate_against_previous_model()
    
    # 4. æ›´æ–°æœ€ä½³æ¨¡å‹
    if win_rate > 0.55:  # 55%èƒœç‡é˜ˆå€¼
        update_best_model()
        print(f"æ¨¡å‹æ›´æ–°ï¼èƒœç‡: {win_rate:.2%}")
    
    return win_rate

def generate_self_play_game():
    """ç”Ÿæˆä¸€å±€è‡ªå¯¹å¼ˆæ•°æ®"""
    game_state = GameState()
    game_examples = []
    
    while not game_state.game_over:
        # MCTSæœç´¢
        action_probs, _ = mcts.search(game_state)
        
        # è®°å½•è®­ç»ƒæ ·æœ¬
        board_state = encoder.encode_board(game_state.board, game_state.current_player)
        example = TrainingExample(
            board_state=board_state,
            action_probs=action_probs,
            value=0.0,  # ä¸´æ—¶å€¼ï¼Œæ¸¸æˆç»“æŸåæ›´æ–°
            player=game_state.current_player
        )
        game_examples.append(example)
        
        # é€‰æ‹©åŠ¨ä½œï¼ˆæ·»åŠ æ¸©åº¦å‚æ•°ï¼‰
        action = sample_action(action_probs, temperature=1.0)
        game_state.make_move(action)
    
    # æ›´æ–°æ‰€æœ‰æ ·æœ¬çš„ä»·å€¼
    final_reward = get_game_result_reward(game_state)
    for i, example in enumerate(game_examples):
        # äº¤æ›¿è§†è§’çš„å¥–åŠ±
        if i % 2 == 0:
            example.value = final_reward
        else:
            example.value = -final_reward
    
    return game_examples
```

### ç¬¬ä¸‰é˜¶æ®µï¼šè‡ªä¸»å¼ºåŒ–è®­ç»ƒ (501-1000è½®)
- **ç›®æ ‡**ï¼šè¶…è¶ŠMCTSæ€§èƒ½
- **æ–¹æ³•**ï¼šçº¯PPOè‡ªå¯¹å¼ˆè®­ç»ƒ
- **ç­–ç•¥**ï¼š
  - ç½‘ç»œè‡ªä¸»å†³ç­–ï¼Œæ— MCTSè¾…åŠ©
  - æŒç»­è‡ªå¯¹å¼ˆäº§ç”Ÿè®­ç»ƒæ•°æ®
  - åŠ¨æ€è°ƒæ•´æ¢ç´¢ç­–ç•¥
- **è¯„ä¼°**ï¼šä¸ä¼ ç»Ÿè±¡æ£‹å¼•æ“å¯¹æ¯”

#### å¼ºåŒ–å­¦ä¹ é…ç½®
```python
reinforcement_config = {
    "learning_rate": 0.001,         # è¾ƒä½å­¦ä¹ ç‡ç¨³å®šè®­ç»ƒ
    "batch_size": 32,               # é€‚ä¸­æ‰¹æ¬¡
    "mcts_simulations": 800,        # MCTSæ¨¡æ‹Ÿæ¬¡æ•°
    "self_play_games": 100,         # æ¯è½®è‡ªå¯¹å¼ˆå±€æ•°
    "training_iterations": 1000,    # è®­ç»ƒè½®æ•°
    "evaluation_games": 20,         # è¯„ä¼°å¯¹å±€æ•°
    "temperature_schedule": {       # æ¸©åº¦è°ƒåº¦
        0: 1.0,      # å‰æœŸé«˜æ¸©åº¦ï¼Œå¢åŠ æ¢ç´¢
        500: 0.5,    # ä¸­æœŸé™æ¸©
        800: 0.1,    # åæœŸä½æ¸©åº¦ï¼Œæ›´ç¡®å®šæ€§
    },
    "c_puct": 1.0,                 # MCTSæ¢ç´¢å¸¸æ•°
}
```

### ç¬¬å››é˜¶æ®µï¼šç²¾è‹±å¯¹æŠ—è®­ç»ƒ (1001+è½®)
- **ç›®æ ‡**ï¼šè¾¾åˆ°ä¸“ä¸šæ°´å¹³
- **æ–¹æ³•**ï¼šä¸å¼ºåŠ›å¯¹æ‰‹å¯¹æˆ˜
- **ç­–ç•¥**ï¼š
  - ä¸å†å²ç‰ˆæœ¬ç½‘ç»œå¯¹æˆ˜
  - å¼•å…¥å¼€å±€åº“å’Œæ®‹å±€åº“
  - ç»†åŒ–ä½ç½®è¯„ä¼°
- **è¯„ä¼°**ï¼šä¸ä¸“ä¸šæ£‹æ‰‹å¯¹æ¯”

## ğŸ“Š æ€§èƒ½è¯„ä¼°æŒ‡æ ‡

### è®­ç»ƒè¿‡ç¨‹ç›‘æ§
1. **æŸå¤±å‡½æ•°**ï¼š
   - ç­–ç•¥æŸå¤± (Policy Loss)
   - ä»·å€¼æŸå¤± (Value Loss)
   - æ€»æŸå¤± (Total Loss)

2. **PPOç‰¹å®šæŒ‡æ ‡**ï¼š
   - ç­–ç•¥ç†µ (Policy Entropy)
   - KLæ•£åº¦ (KL Divergence)
   - è£å‰ªæ¯”ä¾‹ (Clipping Ratio)
   - ä¼˜åŠ¿ä¼°è®¡æ–¹å·® (Advantage Variance)

3. **MCTSé›†æˆæŒ‡æ ‡**ï¼š
   - MCTS-ç½‘ç»œä¸€è‡´æ€§
   - æœç´¢æ•ˆç‡æå‡
   - å†³ç­–æ—¶é—´å¯¹æ¯”

### å¯¹å±€æ€§èƒ½è¯„ä¼°
1. **èƒœç‡ç»Ÿè®¡**ï¼š
   - ä¸ä¸åŒå¼ºåº¦å¯¹æ‰‹çš„èƒœç‡
   - æ‰§çº¢/æ‰§é»‘èƒœç‡å·®å¼‚
   - ä¸åŒå¼€å±€çš„è¡¨ç°

2. **æ£‹å±€è´¨é‡**ï¼š
   - å¹³å‡æ­¥æ•°
   - å¤±è¯¯ç‡åˆ†æ
   - æˆ˜æœ¯ç»„åˆè¯†åˆ«

3. **è®¡ç®—æ•ˆç‡**ï¼š
   - æ¯æ­¥æ€è€ƒæ—¶é—´
   - å†…å­˜ä½¿ç”¨é‡
   - GPUåˆ©ç”¨ç‡

## ğŸ”§ å®ç°æ–‡ä»¶ç»“æ„

```
chess_ai/
â”œâ”€â”€ networks/
â”‚   â”œâ”€â”€ actor_critic_network.py    # Actor-Criticç½‘ç»œå®ç°
â”‚   â”œâ”€â”€ network.py                 # åŸå§‹åŒå¤´ç½‘ç»œ(ä¿ç•™)
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ ppo_trainer.py            # PPOè®­ç»ƒå™¨
â”‚   â”œâ”€â”€ mcts_ppo_trainer.py       # MCTS+PPOæ•´åˆè®­ç»ƒå™¨
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ mcts/
â”‚   â”œâ”€â”€ mcts.py                   # MCTSç®—æ³•å®ç°
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ board_encoder.py          # æ£‹ç›˜ç¼–ç 
â”‚   â”œâ”€â”€ game_utils.py            # æ¸¸æˆå·¥å…·
â”‚   â””â”€â”€ __init__.py
â””â”€â”€ main.py                       # ä¸»è®­ç»ƒè„šæœ¬
```

## ğŸ¯ æ€»ç»“

æœ¬è®¾è®¡æ–¹æ¡ˆæˆåŠŸæ•´åˆäº†ä»¥ä¸‹å…³é”®æŠ€æœ¯ï¼š

1. **MCTSç®—æ³•ä¿ç•™**ï¼šç»§ç»­ä½¿ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ä½œä¸ºæ ¸å¿ƒå†³ç­–ç®—æ³•
2. **PPOè®­ç»ƒç®—æ³•**ï¼šé‡‡ç”¨è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–è¿›è¡Œç¨³å®šçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒ
3. **Actor-Criticæ¶æ„**ï¼šä½¿ç”¨åˆ†ç¦»çš„ç­–ç•¥ç½‘ç»œå’Œä»·å€¼ç½‘ç»œæå‡å­¦ä¹ æ•ˆç‡

### æ ¸å¿ƒä¼˜åŠ¿

- **ç¨³å®šæ€§**ï¼šPPOç®—æ³•ç¡®ä¿è®­ç»ƒè¿‡ç¨‹ç¨³å®šï¼Œé¿å…ç­–ç•¥å´©æºƒ
- **æ•ˆç‡æ€§**ï¼šActor-Criticæ¶æ„æä¾›æ›´å‡†ç¡®çš„ä»·å€¼ä¼°è®¡å’Œç­–ç•¥æ¢¯åº¦
- **å¯æ‰©å±•æ€§**ï¼šæ¨¡å—åŒ–è®¾è®¡ä¾¿äºåç»­ä¼˜åŒ–å’Œæ‰©å±•
- **å®ç”¨æ€§**ï¼šMCTSæä¾›å¼ºå¤§çš„æœç´¢èƒ½åŠ›ï¼Œé€‚åˆå¤æ‚çš„è±¡æ£‹ç¯å¢ƒ

é€šè¿‡è¿™ç§è®¾è®¡ï¼Œæˆ‘ä»¬èƒ½å¤Ÿæ„å»ºä¸€ä¸ªæ—¢ä¿æŒMCTSå¼ºå¤§æœç´¢èƒ½åŠ›ï¼Œåˆå…·å¤‡ç°ä»£æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¼˜åŠ¿çš„ä¸­å›½è±¡æ£‹AIç³»ç»Ÿã€‚

## ğŸ“ˆ è®­ç»ƒè¶…å‚æ•°å’Œä¼˜åŒ–ç­–ç•¥

### 1. å­¦ä¹ ç‡è°ƒåº¦
```python
def get_learning_rate_schedule():
    """å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥"""
    return {
        "type": "cosine_annealing",
        "initial_lr": 0.01,
        "min_lr": 1e-6,
        "warmup_epochs": 10,
        "cosine_cycles": 3,
    }

class CosineAnnealingWithWarmup:
    def __init__(self, optimizer, warmup_epochs, max_epochs, min_lr):
        self.optimizer = optimizer
        self.warmup_epochs = warmup_epochs
        self.max_epochs = max_epochs
        self.min_lr = min_lr
        self.initial_lr = optimizer.param_groups[0]['lr']
    
    def step(self, epoch):
        if epoch < self.warmup_epochs:
            # çº¿æ€§é¢„çƒ­
            lr = self.initial_lr * (epoch + 1) / self.warmup_epochs
        else:
            # ä½™å¼¦é€€ç«
            progress = (epoch - self.warmup_epochs) / (self.max_epochs - self.warmup_epochs)
            lr = self.min_lr + (self.initial_lr - self.min_lr) * 0.5 * (1 + math.cos(math.pi * progress))
        
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr
```

### 2. æ­£åˆ™åŒ–ç­–ç•¥
```python
def apply_regularization(model, config):
    """åº”ç”¨æ­£åˆ™åŒ–ç­–ç•¥"""
    
    # L2æƒé‡è¡°å‡
    optimizer = optim.Adam(
        model.parameters(),
        lr=config['learning_rate'],
        weight_decay=config['weight_decay']
    )
    
    # Dropoutï¼ˆè®­ç»ƒæ—¶ï¼‰
    if config.get('use_dropout', False):
        model.train()  # å¯ç”¨dropout
    
    # æ¢¯åº¦è£å‰ª
    if config.get('gradient_clip', None):
        torch.nn.utils.clip_grad_norm_(
            model.parameters(), 
            config['gradient_clip']
        )
    
    return optimizer
```

### 3. æ•°æ®å¢å¼º
```python
def augment_chess_data(examples):
    """è±¡æ£‹æ•°æ®å¢å¼º"""
    augmented = []
    
    for example in examples:
        # åŸå§‹æ ·æœ¬
        augmented.append(example)
        
        # å·¦å³é•œåƒ
        mirrored_board = mirror_board_horizontally(example.board_state)
        mirrored_actions = mirror_action_probs(example.action_probs)
        augmented.append(TrainingExample(
            board_state=mirrored_board,
            action_probs=mirrored_actions,
            value=example.value,
            player=example.player
        ))
    
    return augmented

def mirror_board_horizontally(board_state):
    """æ°´å¹³é•œåƒæ£‹ç›˜"""
    # æ²¿ç€ä¸­è½´çº¿ï¼ˆç¬¬4åˆ—ï¼‰é•œåƒ
    mirrored = board_state.copy()
    mirrored = mirrored[:, :, ::-1]  # ç¿»è½¬åˆ—
    return mirrored
```

## ğŸ”§ æ¨¡å‹ä¼˜åŒ–å’Œéƒ¨ç½²

### 1. æ¨¡å‹å‹ç¼©
```python
def compress_model(model):
    """æ¨¡å‹å‹ç¼©"""
    
    # é‡åŒ–
    quantized_model = torch.quantization.quantize_dynamic(
        model, {torch.nn.Linear}, dtype=torch.qint8
    )
    
    # å‰ªæ
    import torch.nn.utils.prune as prune
    for module in model.modules():
        if isinstance(module, torch.nn.Conv2d):
            prune.l1_unstructured(module, name='weight', amount=0.2)
    
    # çŸ¥è¯†è’¸é¦
    student_model = create_smaller_network()
    distill_knowledge(model, student_model)
    
    return quantized_model, student_model
```

### 2. æ¨ç†ä¼˜åŒ–
```python
def optimize_inference(model):
    """æ¨ç†ä¼˜åŒ–"""
    
    # è½¬æ¢ä¸ºTorchScript
    scripted_model = torch.jit.script(model)
    
    # å›¾ä¼˜åŒ–
    scripted_model = torch.jit.optimize_for_inference(scripted_model)
    
    # å†…å­˜ä¼˜åŒ–
    scripted_model.eval()
    
    return scripted_model
```

## ğŸ“Š æ€§èƒ½è¯„ä¼°å’Œç›‘æ§

### 1. è®­ç»ƒç›‘æ§æŒ‡æ ‡
```python
training_metrics = {
    "loss": {
        "total_loss": [],
        "policy_loss": [],
        "value_loss": [],
    },
    "accuracy": {
        "policy_accuracy": [],  # ç­–ç•¥é¢„æµ‹å‡†ç¡®ç‡
        "value_mse": [],        # ä»·å€¼é¢„æµ‹è¯¯å·®
    },
    "game_performance": {
        "win_rate": [],         # å¯¹æˆ˜èƒœç‡
        "average_game_length": [], # å¹³å‡å¯¹å±€é•¿åº¦
        "blunder_rate": [],     # å¤±è¯¯ç‡
    },
    "training_efficiency": {
        "samples_per_second": [],
        "gpu_utilization": [],
        "memory_usage": [],
    }
}
```

### 2. æ£‹åŠ›è¯„ä¼°
```python
def evaluate_chess_strength(model):
    """è¯„ä¼°æ£‹åŠ›æ°´å¹³"""
    
    # å¯¹æˆ˜ä¸åŒå¼ºåº¦çš„å¯¹æ‰‹
    opponents = {
        "random": RandomPlayer(),
        "minimax_depth3": MinimaxPlayer(depth=3),
        "minimax_depth5": MinimaxPlayer(depth=5),
        "previous_model": load_previous_model(),
        "human_amateur": HumanPlayer(level="amateur"),
    }
    
    results = {}
    for name, opponent in opponents.items():
        win_rate = play_matches(model, opponent, num_games=100)
        results[name] = win_rate
        print(f"vs {name}: {win_rate:.1%}")
    
    # è®¡ç®—ELOç­‰çº§åˆ†
    elo_rating = calculate_elo_rating(results)
    print(f"ä¼°è®¡ELOç­‰çº§åˆ†: {elo_rating}")
    
    return results, elo_rating
```

## ğŸ¯ é¢„æœŸæ€§èƒ½å’Œé‡Œç¨‹ç¢‘

### è®­ç»ƒé˜¶æ®µç›®æ ‡

#### é˜¶æ®µ1ï¼šç›‘ç£å­¦ä¹  (0-100è½®)
- **ç›®æ ‡**ï¼šå­¦ä¼šåŸºæœ¬è§„åˆ™å’Œå¸¸è§æ¨¡å¼
- **é¢„æœŸæ£‹åŠ›**ï¼šä¸šä½™åˆçº§ (ELO 1200-1400)
- **å…³é”®æŒ‡æ ‡**ï¼š
  - åˆæ³•ç§»åŠ¨ç‡ > 95%
  - åŸºæœ¬æˆ˜æœ¯è¯†åˆ«ç‡ > 70%
  - å¯¹æˆ˜éšæœºç©å®¶èƒœç‡ > 90%

#### é˜¶æ®µ2ï¼šå¼ºåŒ–å­¦ä¹  (100-1000è½®)
- **ç›®æ ‡**ï¼šå‘å±•ç‹¬ç‰¹ç­–ç•¥ï¼Œè¶…è¶Šç›‘ç£å­¦ä¹ 
- **é¢„æœŸæ£‹åŠ›**ï¼šä¸šä½™ä¸­çº§ (ELO 1400-1800)
- **å…³é”®æŒ‡æ ‡**ï¼š
  - å¯¹æˆ˜ç›‘ç£å­¦ä¹ æ¨¡å‹èƒœç‡ > 60%
  - å¹³å‡å¯¹å±€è´¨é‡æ˜¾è‘—æå‡
  - å‘ç°æ–°çš„æˆ˜æœ¯ç»„åˆ

#### é˜¶æ®µ3ï¼šé«˜çº§ä¼˜åŒ– (1000+è½®)
- **ç›®æ ‡**ï¼šè¾¾åˆ°ä¸“ä¸šæ°´å¹³
- **é¢„æœŸæ£‹åŠ›**ï¼šä¸šä½™é«˜çº§-ä¸“ä¸šçº§ (ELO 1800-2200+)
- **å…³é”®æŒ‡æ ‡**ï¼š
  - å¯¹æˆ˜å¼ºåŠ›å¼•æ“æœ‰ç«äº‰åŠ›
  - åœ¨å¤æ‚å±€é¢ä¸‹è¡¨ç°ä¼˜å¼‚
  - å…·å¤‡æ·±åº¦æˆ˜ç•¥æ€ç»´

### è®¡ç®—èµ„æºéœ€æ±‚

#### ç¡¬ä»¶é…ç½®
```python
recommended_hardware = {
    "GPU": "NVIDIA RTX 4090 æˆ–æ›´é«˜",
    "VRAM": "24GB+",
    "CPU": "16æ ¸å¿ƒä»¥ä¸Š",
    "RAM": "64GB+",
    "å­˜å‚¨": "2TB SSD",
}

training_time_estimates = {
    "ç›‘ç£å­¦ä¹ ": "2-3å¤©",
    "å¼ºåŒ–å­¦ä¹ 1000è½®": "2-3å‘¨",
    "å®Œæ•´è®­ç»ƒ": "1-2ä¸ªæœˆ",
}
```

## ğŸ“ æ€»ç»“

æœ¬è®¾è®¡æ–¹æ¡ˆæä¾›äº†ä¸€ä¸ªå®Œæ•´çš„ä¸­å›½è±¡æ£‹AIç¥ç»ç½‘ç»œæ¶æ„ï¼ŒåŒ…æ‹¬ï¼š

1. **å®Œæ•´çš„ç½‘ç»œæ¶æ„**ï¼šåŸºäºResNet+æ³¨æ„åŠ›æœºåˆ¶çš„åŒå¤´ç½‘ç»œ
2. **ç²¾å¿ƒè®¾è®¡çš„å¥–åŠ±å·¥ç¨‹**ï¼šå¤šç»´åº¦å¥–åŠ±å‡½æ•°ï¼Œå¹³è¡¡æè´¨ã€ä½ç½®ã€æˆ˜æœ¯ä»·å€¼
3. **åˆ†é˜¶æ®µè®­ç»ƒç­–ç•¥**ï¼šä»ç›‘ç£å­¦ä¹ åˆ°å¼ºåŒ–å­¦ä¹ çš„æ¸è¿›å¼è®­ç»ƒ
4. **å…¨é¢çš„ä¼˜åŒ–æ–¹æ¡ˆ**ï¼šå­¦ä¹ ç‡è°ƒåº¦ã€æ­£åˆ™åŒ–ã€æ•°æ®å¢å¼ºç­‰
5. **è¯¦ç»†çš„è¯„ä¼°ä½“ç³»**ï¼šå¤šç»´åº¦æ€§èƒ½ç›‘æ§å’Œæ£‹åŠ›è¯„ä¼°

è¯¥æ–¹æ¡ˆç»“åˆäº†ç°ä»£æ·±åº¦å­¦ä¹ çš„æœ€ä½³å®è·µå’Œä¸­å›½è±¡æ£‹çš„é¢†åŸŸçŸ¥è¯†ï¼Œé¢„æœŸèƒ½å¤Ÿè®­ç»ƒå‡ºå…·æœ‰ä¸“ä¸šæ°´å¹³çš„è±¡æ£‹AIã€‚é€šè¿‡åˆ†é˜¶æ®µè®­ç»ƒå’ŒæŒç»­ä¼˜åŒ–ï¼Œæ¨¡å‹å°†é€æ­¥ä»åŸºç¡€è§„åˆ™å­¦ä¹ å‘å±•åˆ°é«˜çº§æˆ˜ç•¥æ€ç»´ï¼Œæœ€ç»ˆè¾¾åˆ°ç”šè‡³è¶…è¶Šäººç±»ä¸“å®¶çš„æ°´å¹³ã€‚